<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Node.js | Razor Sharp Design]]></title>
  <link href="http://rzrsharp.net/blog/categories/node-dot-js/atom.xml" rel="self"/>
  <link href="http://rzrsharp.net/"/>
  <updated>2013-07-25T17:29:55-07:00</updated>
  <id>http://rzrsharp.net/</id>
  <author>
    <name><![CDATA[Justin Reidy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Testing and Node: Building a Toolset]]></title>
    <link href="http://rzrsharp.net/2013/07/25/testing-and-node-building-a-toolset.html"/>
    <updated>2013-07-25T16:48:00-07:00</updated>
    <id>http://rzrsharp.net/2013/07/25/testing-and-node-building-a-toolset</id>
    <content type="html"><![CDATA[<p><em>This is part two of a series on testing and Node; check out part one,
<a href="/2013/07/16/testing-and-node-why-do-we-test.html">Why do we test</a>.</em></p>

<p>Despite Node.js&rsquo;s relative youth, there&rsquo;s already a  vast ecosystem of tooling
surrounding end-to-end application testing. But the bounteous availability of tools isn&rsquo;t
as positive an aspect as it may appear; many tools lack maturity, while others
have gone beyond the point of maturity to abandonware. Choosing the right
testing toolset when starting a new Node project can be daunting, and can be
enough to discourage you from testing in the first place.</p>

<p>Don&rsquo;t delay. Don&rsquo;t put off. Follow the advice of the excellent
<a href="http://www.growing-object-oriented-software.com/">Growing Object-Oriented Software, Guided by Tests</a>
and set up your test infrastructure
when you first set up your application.</p>

<p>Today, we&rsquo;re going to go step-by-step through creating a new Node app and its accompanying test toolset.
Over the rest of the series, we&rsquo;ll walk through writing a
feature from the outside-in.</p>

<p>Let&rsquo;s get started. Create a new directory for this project; I created one
called <code>testing-and-node</code>. <code>cd</code> in and then <code>npm init</code>. If you haven&rsquo;t used
<code>npm init</code> before, it just asks a series of questions to put together a
<code>package.json</code> file for you. Fill in whatever answers you want, but when the
script asks for the &ldquo;test command&rdquo;, make sure to enter <code>grunt test</code>.</p>

<p>Speaking of which&hellip; you do have <a href="http://gruntjs.com/">Grunt</a> installed, right?
Grunt is a fantastically powerful build tool for JS projects. While some
developers prefer the power and unix-ness of Makefiles, and Rubyists long for
Rake&rsquo;s API, Grunt has a comprehensive ecosystem of tasks that&rsquo;s growing daily.
And since you&rsquo;re coding in JavaScript, you might as well write your build tasks
in JS, too.</p>

<p>If you <em>are</em> new to Grunt, you&rsquo;ll need to install the cli module for use
globally:</p>

<p><code>bash
npm install -g grunt-cli
</code></p>

<p>Next, install a local version of Grunt itself:</p>

<p><code>bash
npm install --save grunt
</code></p>

<p>Great. Grunt relies on the presence of a file called <code>Gruntfile.js</code> to
configure and run builds. Let&rsquo;s set that up now. Open a new Gruntfile.js file
in your editor and enter the following:</p>

<p>```javascript
module.exports = function (grunt) {</p>

<p>  grunt.initConfig({</p>

<p>  });</p>

<p>  grunt.registerTask(&lsquo;test&rsquo;, []); };
```</p>

<p>Go ahead and run <code>npm test</code>. It&rsquo;s green. Hooray! Yes, it&rsquo;s true your tests
don&rsquo;t currently do anything, but now you&rsquo;re free to start setting up the rest
of your toolset. Let&rsquo;s install them in one bunch:</p>

<p><code>bash
npm install --save-dev mocha sinon chai sinon-chai grunt-mocha-test grunt-mocha-webdriver
</code></p>

<p>And if you&rsquo;re a fan of
<a href="http://domenic.me/2012/10/14/youre-missing-the-point-of-promises/">promises</a>,
you might as well install the following right now:</p>

<p><code>bash
npm install --save Q
npm install --save-dev chai-as-promised
</code></p>

<p>We&rsquo;ll also need to install a tool that&rsquo;s not a Node module:
<a href="http://phantomjs.org/">PhantomJS</a>.  You can install it with
<a href="http://brew.sh/">Homebrew</a> if you&rsquo;ve got it; otherwise, there&rsquo;s
<a href="http://phantomjs.org/download.html">binary installers here</a>. If you already have
Phantom installed, just make sure it&rsquo;s on the latest version.</p>

<p>Let&rsquo;s go over the selection of each of these tools. There&rsquo;s certainly other
sets of modules that would work equally well, but this is the toolset with
which I&rsquo;m most familiar, and the one for which I&rsquo;ll advocate.</p>

<h3>mocha <a href="http://visionmedia.github.io/mocha/">http://visionmedia.github.io/mocha/</a></h3>

<p>There&rsquo;s a wide variety of testing frameworks available for Node, from the
extraordinarily comprehensive (<a href="http://pivotal.github.io/jasmine/">Jasmine</a>) to
the extraordinarily minimal (<a href="https://github.com/isaacs/node-tap">node-tap</a>).
Mocha fits neatly in between these two extremes; its one role is to run tests,
and it&rsquo;s agnostic about output, assertion library, and even interface-style
(the only thing Mocha <em>isn&rsquo;t</em> agnostic about is <a href="https://github.com/visionmedia/mocha/pull/329">promise
support</a>). Mocha&rsquo;s best feature
is its ease of async use; supply a test with a callback, and it will be run
asynchronously.</p>

<p>```javascript
describe(&lsquo;a sync test&rsquo;, function () {
  it(&lsquo;is run synchronously&rsquo;, function () {</p>

<pre><code>//assert something synchronous
</code></pre>

<p>  });
});</p>

<p>describe(&lsquo;an async test&rsquo;, function () {
  it(&lsquo;is run asynchronously&rsquo;, function (done) {</p>

<pre><code>//do something async
done(err, result);
</code></pre>

<p>  });
});
```</p>

<h3>sinon <a href="http://sinonjs.org/">http://sinonjs.org/</a></h3>

<p>Since Mocha simply plays the role of test-runner, we need other components to complete our testing
toolset. Sinon is a framework-agnostic library that provides spies, stubs, and
mocks for JavaScript
(<a href="http://martinfowler.com/articles/mocksArentStubs.html#TheDifferenceBetweenMocksAndStubs">Martin Fowler</a>
provides a useful explanation of those terms). Spies, stubs, and mocks are
integral for properly testing the interfaces through which your application
objects communicate; Sinon makes it simple to verify how any function is
called, and can even indicate whether a function has been new-ed.</p>

<h3>chai <a href="http://chaijs.com/">chaijs.com</a></h3>

<p>Completing our main set of testing
tools is chai, a framework-agnostic assertion library. Node provides
<a href="http://nodejs.org/api/assert.html">its own assertion api</a> directly in core, so it&rsquo;s not
strictly necessary to include a third-party assertion module. But Chai provides
a choice of excellent APIs, with error messages that should make the path from
red to green much clearer. I&rsquo;m personally partial to Chai&rsquo;s
<a href="http://chaijs.com/api/bdd/">expect</a> bdd interface. Another benefit of Chai is
that it provides an easy mechanism for adding your own assertion language,
which has led to assertion-plugins like sinon-chai and chai-as-promised.</p>

<h3>sinon-chai <a href="http://chaijs.com/plugins/sinon-chai">http://chaijs.com/plugins/sinon-chai</a></h3>

<p>I&rsquo;m a big believer that cleaner APIs lead to better code; while it&rsquo;s perfectly
acceptable to test Sinon object properties directly, the sinon-chai plugin
makes your tests even clearer. Instead of:</p>

<p><code>javascript
expect(mySpy.calledWith("foo")).to.be.ok;
</code></p>

<p>you can just write:</p>

<p><code>javascript
expect(mySpy).to.have.been.calledWith("foo");
</code></p>

<p>Since you&rsquo;re
going to the trouble of using an expressive assertion library, it makes sense
to make those assertions as clear as possible.</p>

<h3>chai-as-promised <a href="http://chaijs.com/plugins/chai-as-promised">http://chaijs.com/plugins/chai-as-promised</a></h3>

<p>For promise-users, chai-as-promised provides the same benefits as sinon-chai:
more expressive test assertions. Instead of chaining a promise onto the subject
of your test to verify the eventual resolution or rejection, chai-as-promised
allows you to use the same chaining-API as the rest of Chai:</p>

<p><code>javascript
expect(promiseFn({foo: 'bar'})).to.eventually.deep.equal('foobar')
</code></p>

<h3>grunt-mocha-test <a href="https://github.com/pghalliday/grunt-mocha-test">https://github.com/pghalliday/grunt-mocha-test</a></h3>

<p>Because Mocha can be run on both the client and server, there&rsquo;s a surprisingly
large number of Grunt tasks that use the testing framework; I prefer
grunt-mocha-test because of its extreme simplicity. grunt-mocha-test uses the
same options hash as Mocha itself; in fact, all it&rsquo;s doing is firing up processes
with which to run Mocha instances. Of course, running those instances properly,
and catching any weird exceptions, is harder than it sounds &mdash; grunt-mocha-test
has figured out those fringe cases so you don&rsquo;t have to.</p>

<h3>grunt-mocha-webdriver <a href="https://github.com/jmreidy/grunt-mocha-webdriver">https://github.com/jmreidy/grunt-mocha-webdriver</a></h3>

<p>When it comes down to full-stack (aka acceptance or end-to-end) testing, you&rsquo;ll
need a headless browser with which to interact. PhantomJS, which we&rsquo;ll get to
below, provides a headless WebKit browser, but what if you want to run your
acceptance tests against more than just a (fake) instance of WebKit? What if you
want to know if your application will work on mobile browsers, or legacy
versions of IE? That&rsquo;s where <a href="https://saucelabs.com/">SauceLabs</a> comes into
play; it provides an IaaS for testing against any matrix of browsers and
devices.</p>

<p>grunt-mocha-webdriver allows you to write and run Mocha tests as you normally
would, but with one huge benefit &mdash; it automatically inserts a
<a href="https://github.com/admc/wd">WebDriver/Selenium</a> interface into your tests for you. With
grunt-mocha-webdriver, you&rsquo;ll be able to run a single acceptance test against
PhantomJS locally, and then run against a whole suite of browsers on SauceLabs
when you deploy to your CI environment.</p>

<p>If all of that sounds incredibly confusing now, it will make more sense in a
future post in this series.</p>

<h3>PhantomJS <a href="http://phantomjs.org/">http://phantomjs.org/</a></h3>

<p>PhantomJS isn&rsquo;t a Node module; it&rsquo;s a headless WebKit browser. Traditionally,
the interface through which you&rsquo;d interact with Phantom would be a JavaScript
API, and it was notoriously tricky to setup and integrate into the rest of the
testing stack. All that changed with PhantomJS&rsquo;s 1.8 release, which included
<a href="https://github.com/detro/ghostdriver">Ghost Driver</a>,
a pure JavaScript implementation of the WebDriver / Selenium Wire Protocol.
With new versions of PhantomJS, you can run:</p>

<p><code>bash
phantomjs --webdriver=4444
</code></p>

<p>That will keep an instance of Phantom running in the background, against which
you can run your tests via grunt-mocha-webdriver.</p>

<p>This whole process will be explained more in a later post in this series.</p>

<hr>


<br />


<p>And with that, we have a working testing toolset. We&rsquo;re ready to build &mdash; which
we&rsquo;ll start doing in the next post.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing and Node: Why Do We Test?]]></title>
    <link href="http://rzrsharp.net/2013/07/16/testing-and-node-why-do-we-test.html"/>
    <updated>2013-07-16T16:30:00-07:00</updated>
    <id>http://rzrsharp.net/2013/07/16/testing-and-node-why-do-we-test</id>
    <content type="html"><![CDATA[<p>Why do we test our code?</p>

<p>The question itself seems sacrilegious in today&rsquo;s developer culture. Admitting
that you aren&rsquo;t building a comprehensive suite of unit and integration tests,
let alone not testing <em>first</em>, amounts to hanging a giant &ldquo;newb&rdquo; sign around
your neck. But sacred cows in any culture can lead to stagnation and a lack of
innovation; if you can&rsquo;t provide good cause for a practice that&rsquo;s demanded of
you, why bother doing it?</p>

<p>I&rsquo;ve worked on a number of different projects, spanning maturity levels,
technologies, and team sizes. While most have employed some degree of testing,
I can&rsquo;t think of a single one that employed what would be considered a &ldquo;robust
test suite&rdquo;. Certainly, open source libraries to which I&rsquo;ve contributed (and
the ones I&rsquo;ve created) have almost all included solid unit test coverage. But
there&rsquo;s a gap between what many product teams and consulting agencies preach
(&ldquo;We test all our code all the time, and only want developers who believe in
testing!&rdquo;) and what they practice (&ldquo;We&rsquo;ve got to get this feature out the door,
forget the tests for now.&rdquo;).</p>

<p>What makes the testing question more complicated is that you can never
fully-automate your way to safety. No matter how comprehensive your tests,
you&rsquo;re going to need a QA team. The most glaring need for QA is the fact that
you can&rsquo;t test the appearance of an app &mdash; and with the rise of the responsive
web, testing appearance (and by extension, usability) becomes even more
important. If you need to spend resources on QA-ing an app anyway, why should
your development team spend extra time writing tests (or testing first) when
they could instead be building features or fixing bugs?</p>

<p>The most compelling answer I&rsquo;ve found to this question comes from Sandi Metz in
her book <a href="http://www.amazon.com/Practical-Object-Oriented-Design-Ruby-Addison-Wesley/dp/0321721330/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1374015857&amp;sr=1-1">Practical Object-Oriented Design in Ruby</a>.
It&rsquo;s probably not surprising that a strong case for testing comes from a
Rubyist; the Ruby community has been loudly preaching test-first Agile
methodologies for years. But Metz articulates ideas in a way that increases
their poignancy. &ldquo;Tests give you confidence to refactor constantly,&rdquo; she
writes. This idea takes the common catchphrase of testing &mdash; &ldquo;red, green,
refactor&rdquo; &mdash; and turns it on its head. Instead of refactoring being a <em>part</em> of
testing, refactoring becomes the <em>point</em> of testing. &ldquo;The true purpose of
testing,&rdquo; Metz argues, &ldquo;is to reduce costs.&rdquo;</p>

<p>As a Node developer, the idea that testing provides you freedom to refactor is
entirely compelling. As I wrote in <a href="http://rzrsharp.net/2013/07/02/private-github-repos-with-npm-and-heroku.html">my article on using private Github
repositories for web application
submodules</a>,
Node developers are constantly on the prowl to refactor common bits of logic
into their own encapsulated components. By test-driving feature development,
you make it significantly easier to break out common modules:</p>

<ol>
<li>Test from the outside in: integration, then unit 2. Implement the feature
(make the tests pass) 3. Isolate code that&rsquo;s useful elsewhere, break it off
into its own module, confirm the tests still pass.</li>
</ol>


<p>In Node-land, &ldquo;red-green-refactor&rdquo; becomes &ldquo;red-green-refactor-publish
[module]&rdquo;.</p>

<p>Further, building proper test coverage may require some additional time from
your development team, but it should also decrease the pressure on your QA team
&mdash; and help everyone involved by reducing coordination costs. Think of a
scenario in which one developer bumps up an application dependency to a newer
version in order to implement a new feature (or fix a bug). The developer
sanity checks the application to see if the new version causes any problems
elsewhere in the app; finding none, the updated dependency is added to the
app&rsquo;s manifest and deployed. A week later, a bug report comes in that&rsquo;s vetted
and validated by a member of the QA team, who then needs to coordinate with
multiple developers to identify the cause of the bug (a regression from the
bumped dependency), fix it, and confirm the fix. The cost of coordinating a fix
to this regression clearly outweighs the cost of writing tests in the first
place, and this scenario is of the sort that automated testing can easily
address. In the Node ecosystem, where we frequently rely on many small
third-party modules, it&rsquo;s especially important to automatically test that our
integration with these modules is always working.</p>

<p>But testing can&rsquo;t reduce costs if we aren&rsquo;t efficient testers. And being an
efficient tester is only partially dependent on your knowledge of good OO
design; it is equally important to have a full and proper understanding of the
tools with which you need to test.</p>

<p>Which brings me to the point of this post. The Node community has a very
effective toolbox for testing first, testing well, and testing comprehensively.
But despite the capabilities of these tools, there&rsquo;s very little discussion on
the web of how to use them. An introduction to Mocha or Jasmine doesn&rsquo;t really
explain how to use them to TDD a Node app, and it certainly doesn&rsquo;t explain how
to integrate these libraries into a continuous-deployment setup with Grunt.</p>

<p>In the next few posts, I&rsquo;ll be stepping through my own &ldquo;best practices&rdquo; for
testing a Node app. I&rsquo;ll be the first to admit that there are probably better
approaches to testing, and I&rsquo;ll be quite happy to receive feedback, but
hopefully this series can serve as a guide to get the Node testing discussion
started. If there&rsquo;s anything you&rsquo;re particularly curious about, please let me
know and I&rsquo;ll try to address it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[In Theory, Not Practice]]></title>
    <link href="http://rzrsharp.net/2013/04/11/in-theory-not-practice.html"/>
    <updated>2013-04-11T19:49:00-07:00</updated>
    <id>http://rzrsharp.net/2013/04/11/in-theory-not-practice</id>
    <content type="html"><![CDATA[<p>In <a href="http://rzrsharp.net/2013/04/02/towards-a-unified-js-stack.html">my previous post</a>, I wrote about the quest towards unifying web application architecture around a single shared codebase. <a href="https://github.com/airbnb/rendr">Airbnb’s Rendr library</a> provides one path forward on that quest. In this post, I’d like to provide a high-level explanation of my own solution for unifying application code.</p>

<p>There’s a few segments of an application’s code that will almost certainly be repeated on both the client and server: views, view state, and data models (excluding data access). To a certain extent, routing could be repeated as well; if you’re not using hashbangs, you’ll need to implement the same named routes on the client and server (although the actions you perform for those routes will be dramatically different).</p>

<p>The solution I’ve put together is still a work in progress, and is more a set of practices and conventions than a library. This flexibility means that while I’ve been implementing the approach using Handlebars, Backbone, and Express, you could theoretically use a completely different set of libraries. In this post, I’ll go over the basic ideas behind my approach; I’ll be putting together a sample implementation on Github that I’ll be able to write about in more detail in the future.</p>

<h2>Unifying View Templates</h2>

<p>Sharing template code is perhaps the easiest step towards achieving web application unity. With a full-JS stack, you’re compiling the same templates for rendering both server-side and client-side. So why not use the same templates for each?</p>

<p>In my experiments, I’ve been using <a href="http://handlebarsjs.com/">Handlebars</a> as my templating system. It’s based on the Mustache syntax, but has a number of powerful features, including partials and custom helpers. Handlebars is also the templating system behind Ember.js.</p>

<p>The key to sharing templates is the realization that what constitutes a “partial” can be relative to where the template is used — that is to say, the same template can be used as a full template on the client, and a partial template on the server. My templates are divided into three groups:</p>

<ul>
<li>client &ndash; client only templates</li>
<li>shared &ndash; used on both the client and server</li>
<li>server &ndash; server only templates</li>
</ul>


<p>For the client, the templates in the client and shared folders are pre-compiled in a Grunt step, so for all intents and purposes there’s a single flat collection of templates. The server, however, references all of the templates in the shared folder as partials. What’s more, view helpers (in this case, special Handlebars tags) are stored in a <code>shared/helpers</code> directory that is referenced by both the client (in a Grunt step) and the server.</p>

<p>If you’re confused now, you won’t be when you see the actual implementation in my next post. What sounds complicated here is quite straightforward in practice.</p>

<h2>Populating View Templates</h2>

<p>If it’s easy to share templates on the client and server, getting the right data into those templates is a bit of a harder proposition. Handlebars renders data by running the template against a “context”, which is just a plain old hash. But how can we make sure that the contents of that hash are synchronized when rendering the same view on both the client and server?</p>

<p>The problem is exacerbated with how Backbone muddles view rendering logic. In Backbone, there’s not a fine line between the View logic (in Backbone.View) and Model logic (in Backbone.Model and Backbone.Collection). Backbone Views end up performing view state management by manipulating model instances, in order to avoid bloating Models with View specific code. But adding stateful logic into a Backbone View causes two problems:</p>

<p>1) it makes for larger, more complex views, as concerns are split between UX event management and controlling model state;</p>

<p>2) it makes it harder to share code with the server, as rendering state gets locked in a Backbone view.</p>

<p>We can get around these problems by reconsidering the role of a Backbone View — separating its concerns into that of Mediator and View Model. The Mediator role is one of event manager and template marshal; the view model is, most basically, a state controller.</p>

<p>By splitting the state controller into an object of its own concern, you can extract the logic into a module that can be shared by both the client (via Browserify) and the server (via require). The Backbone Mediator simply invokes the View Model, declaratively calling functions on it as users interact with the app — transitioning Backbone away from its traditional role of MVC framework into more of <a href="http://addyosmani.com/blog/understanding-mvvm-a-guide-for-javascript-developers/">an MVVM implementation</a>.</p>

<p>A View Model just returns a hash with the current state of data, which is supplied to the template. On the server, a View Model is instantiated with data accessed from the backend; the results of the View Model instance’s <code>toJSON</code> method are passed to the template for rendering. On the client, the same pattern is repeated, but the data is accessed via AJAX instead.</p>

<h2>Sharing Models on Client and Server</h2>

<p>Synchronizing model code on both the client and server is perhaps the hardest step towards achieving web application unity. The challenge primarily derives from a characteristic of Model code that we’ve inherited from the Rails world: specifically, a conflation of Model concerns between “data model object” and “data access object”. A data model object should identify a model object’s properties, its getters and setters, its relationships, and some convenient accessors; a data access object should deal with querying data sources for populating the data model objects.</p>

<p>Unfortunately, Backbone follows in the tradition of Rails’ ActiveRecord, flattening data accessing and data modeling into a single object. This flattening causes problems even beyond unifying a web application stack. How many Backbone applications have you seen that globally override the sync method, and use properties on model classes to determine how sync invocation works? (Not to pick on Backbone; it’s just the web application framework with which I’m most familiar.)</p>

<p>For now, I’ve abandoned trying to get Backbone Models and Collections to work on both the client and server. But there’s alternatives that can fill the gap nicely. The first of which I’m familiar is <a href="https://github.com/dandean/tubbs">Dan Dean’s Tubbs library</a>. Tubbs provides a DSL for defining data model objects, but also provides a pluggable dataStore. A single model module can be defined in a shared folder, and then required on both the client and server, with both supplying their own dataStore.</p>

<p>In order to make the separation between data modeling and data accessing even clearer, and the sharing of model code on the server and client easier, I created by own <a href="https://github.com/jmreidy/voltron-model/tree/0.1.x">voltron-model</a> library. While Voltron isn’t ready for primetime (yet), it should make the unified definition of data models significantly easier.</p>

<p>Whichever modeling library is chosen, the basic approach should be the same: data models are defined in a shared folder. These models are then imported by code on the server (or packaged with Browserify for the client). The server and client should decorate the models appropriately (either by adding to the model’s prototype or adding an accessor component). Thus, a model can be made to access Mongo or Postgres on the backend, or a REST service on the client.</p>

<p>The benefit of using Backbone on the client is that it doesn’t care if you only use limited parts of the library. Arrays of event-emitting models can provide much of the same benefit of Backbone Collections and Models, and Backbone Views and Routers see no difference.</p>

<h2>Wrapping Up</h2>

<p>All this discussion may be a bit confusing without a concrete implementation to reference; hopefully, I’ll be able to put together a sample application in the next few weeks. But the basic takeaway of the post is that unification of JS web applications can be made significantly easier with two factors:</p>

<ul>
<li><p>structuring your code so that it can be required by the server or packaged for the client in the same ways, whether by ingesting templates as partials or packaging code with Browserify;</p></li>
<li><p>following the Node.js approach of writing small, single-purpose modules means that there’s less boilerplate to prevent you from using code in multiple environments</p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Towards a Unified JS Stack]]></title>
    <link href="http://rzrsharp.net/2013/04/02/towards-a-unified-js-stack.html"/>
    <updated>2013-04-02T19:49:00-07:00</updated>
    <id>http://rzrsharp.net/2013/04/02/towards-a-unified-js-stack</id>
    <content type="html"><![CDATA[<p>It’s been a long time since I’ve written anything for this site. Neither neglect nor procrastination prompted my absence, but rather, lots and lots of coding. All that code has left me with a surfeit of topics to cover here, so instead of talking about what I’ve been working on, I’m going to dive straight into the good stuff.</p>

<p>I want to write about the current “Holy Grail” of web application development: a unified application stack. Any dev who’s built a thick-client web app (regardless of library &ndash; Ember, Backbone, this problem is independent of tool choice) has dealt with the frustration of recreating logic in multiple parts of the application stack. It’s incredibly annoying to be writing the same code to handle model accessors, view state, and rendering logic in two different places; it’s even more irksome to be fixing bugs in one or both areas as your application matures. But let’s back up a bit. Why do we need to be writing the same logic in multiple places at all?</p>

<p>Generally speaking, modern web app development requires a choice from one of three architectural thrusts:</p>

<ul>
<li><p>thick server, thin client: view logic is written on the server, and rendering happens there as well. The client code deals with interactivity and AJAX requests, but is mostly just updating areas of the page with rendered view content returned by the server. This approach is <a href="http://37signals.com/svn/posts/3112-how-basecamp-next-got-to-be-so-damn-fast-without-using-much-client-side-ui">most famously advocated for by 37 Signals</a>. A thick server simplifies view code, but does so at the expense of larger AJAX payloads; why pull fully rendered code when you can just pull JSON data and render it with a client-side template?</p></li>
<li><p>thick client, thin server: This architecture inverts the first approach; the server provides an API, which is basically just a wrapper around data access and caching. All of the app logic resides in the client. This approach can make for quick interactivity and rendering on the client, but presents a massive problem with “first load” delay, as the client needs to make two connections to the server: the first to load the client application code, and the second to pull the appropriate data to render. You may remember this problem from <a href="http://engineering.twitter.com/2012/05/improving-performance-on-twittercom.html">Old New Twitter</a>.</p></li>
<li><p>thick client, thick server: Unsurprisingly, this approach combines both of the previous options. Views are rendered on the server, which means the initial load is fast, but the client handles all the rendering from that point forward. The good news is that you have two fairly independent, (hopefully) highly performant application structures; the bad news is that you’re now repeating logic in view templates, view states, model helpers, even route handlers.</p></li>
</ul>


<p>One exciting prospect of Node.js is that it makes Javascript an “isomorphic language”, which is to say that “any given line of code (with notable exceptions) can execute both on the client and the server” (h/t <a href="http://blog.nodejitsu.com/scaling-isomorphic-javascript-code">Nodejitsu</a>). But that said, there haven’t been very compelling demonstrations that solve the code-duplication problem.</p>

<p>In the last few months, I’ve come up with a solution that allows for code reuse on client and server, leading to a unified codebase for handling view rendering, view states, and model interaction. It’s using components and libraries that you already know: Browserify, Backbone, Express, and Handlebars. But I’m going to hold off on describing that solution for now, and instead write about Rendr.</p>

<p>Rendr <a href="http://nerds.airbnb.com/weve-launched-our-first-nodejs-app-to-product">is the result of Airbnb’s first Node app</a>, and it’s also the library that finally prompted me to close Vim and start writing here again. It’s the fulfillment of an idea that I first heard about from <a href="https://github.com/tbranyen">Tim Branyen</a>: what if we used Express to render the same Backbone Views that we render on the client? That question is what prompted me on my own path towards application stack unity, so I’ve been very eager to learn more about Rendr.</p>

<p>Rendr took Tim Branyen’s initial idea and made it a reality. It’s important to note that Airbnb’s experiments here weren’t an excuse for architecture aeronautics; rather, they were expressly concerned with solving the “time to content” problem, which is to say, the rendering delay that Twitter famously faced. And by unifying client and server code, Rendr not only makes developers’ lives easier, it solves that time to content problem.</p>

<p>Rendr has a number of features and design approaches that I very much like:</p>

<ul>
<li><p>it’s described as “a library, not a framework”</p></li>
<li><p>it doesn’t create a server side DOM</p></li>
<li><p>it doesn’t try to collapse backend data access into the client tier, as some other frameworks have done</p></li>
<li><p>it utilizes existing, familiar code (Express and Backbone)</p></li>
<li><p>it keeps the rendered DOM on first load and attaches client objects to it, rather than instantiating client views and replacing the DOM with the results of those client views.</p></li>
</ul>


<p>This last item is particularly important: rendering a page on the server, sending it to the client, and then recreating the DOM on the client can lead to odd rendering issues, interactivity problems, and problems with event listeners. But dynamically instantiating client objects and attaching them to an already-rendered DOM is not a trivial problem, and I commend <a href="https://github.com/spikebrehm">Spike Brehm</a> and the rest of the Airbnb team for taking the time to get this right.</p>

<p>You should definitely take the time to <a href="https://github.com/airbnb/rendr">explore Rendr’s codebase</a>. If it existed when I first set out to solve this same set of problems last year, then I probably would’ve abandoned my own quest and just used this library. But the key point of differentiation with the approach I took from Rendr’s design is modularity. Is it possible to achieve application stack unity, while using objects and code that can be implemented with any number of frameworks in any number of ways?</p>

<p>Stay tuned.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Error Isolation with Promises]]></title>
    <link href="http://rzrsharp.net/2012/10/01/error-isolation-with-promises.html"/>
    <updated>2012-10-01T09:39:00-07:00</updated>
    <id>http://rzrsharp.net/2012/10/01/error-isolation-with-promises</id>
    <content type="html"><![CDATA[<p>The entirety of the Javascript and Node.js universe is based around a
continuation-passing coding style &mdash; that is to say, callbacks. This style of
async coding greatly aggrevates Node critics, who constantly argue that
continuation-passing invariably results in &ldquo;callback hell&rdquo;
spaghetti code for any non-trivial problem. Node.js proponents respond that
continuation-passing presents an easier mental model for async coding
than alternative approaches, and that properly abstracted code avoids any
gross &ldquo;chains of doom.&rdquo;</p>

<p>This post has nothing to do with that argument.</p>

<p>If you&rsquo;re coding in Javascript, you&rsquo;re going to be using callbacks, and if you&rsquo;re
building a library, you&rsquo;re going to be supporting callbacks. It&rsquo;s the reality
of the ecosystem. But there are certain problems for which some syntactic sugar is helpful.
<a href="http://wiki.commonjs.org/wiki/Promises">Promises</a> provide a wrapper for callbacks that
allow for a chainable API around a sequence of continuations. They&rsquo;re not a native part of
the language, but there&rsquo;s <a href="http://wiki.ecmascript.org/doku.php?id=strawman:concurrency#promises_and_promise_states">a strawman</a>
for the inclusion of Promises in a future version of JS. In the meantime, you can use
the excellent <a href="https://github.com/kriskowal/q">Q Library</a> from
<a href="https://github.com/kriskowal">Kris Kowal</a>.
jQuery includes its own Promises implementation (<code>$.ajax</code> calls return Promises), but they&rsquo;re slightly
divergent from the Q spec. You can read <a href="https://github.com/kriskowal/q/wiki/Coming-from-jQuery">this excellent guide</a>
explaining the differences between jQuery and Q promises, if you&rsquo;re coming from one direction
or the other.</p>

<p>So much for introductions. I&rsquo;ve been using promises for awhile now in some of my
Node code, and there&rsquo;s several cases where their inclusion can clean up an API. (One
area: Mongo integration, where any simple action can require several callbacks.) One benefit
of Promises is that they can provide exception isolation, just like continuation-passing.
<a href="https://github.com/kriskowal/q/wiki/On-Exceptions">The Q wiki</a> provides the following
example:</p>

<p><code>javascript
var info, processed;
try {
  info = JSON.parse(json);
  processed = process(info);
} catch (exception) {
  console.log("Information JSON malformed.");
}
</code></p>

<p>In the above snippet, errors generated from the parser and the process function arne&rsquo;t isolated.
Promises, meanwhile, offer this exception isolation out of the box:</p>

<p><code>javascript
var processed = Q.call(function (json) {
  return JSON.parse(json);
})
.then(function (info) {
  return process(info);
}, function (exception) {
  console.log("Information JSON malformed.");
})
</code></p>

<p>In the second example, the process function is only executed if <code>JSON.parse</code>
doesn&rsquo;t throw an error. Conversely, this same goal could be achieved
with continuation-passing:</p>

<p>``` javascript
var processFn = function(cb) {
  asyncJSONParser(json, function(err, info) {</p>

<pre><code>if (err) {
  console.log("Information JSON malformed.");
  return cb(err);
}
cb(null, process(info));
</code></pre>

<p>  });
};
```</p>

<p>One caveat is that when you&rsquo;re chaining your promises together, the behavior is slightly
different than you might expect:</p>

<p>``` javascript
var existingUser;
var promise = User.findOne({username: username})
  .then(function (user) {</p>

<pre><code>if (!user) {
  throw new Error("Username is incorrect");
}
existingUser = user;
return User._hashPassword(password, user.get('salt'));
}, function(error) {
  console.error('error in findOne');
  throw error;
</code></pre>

<p>  })
  .then(function (hashedPassword) {</p>

<pre><code>if (existingUser.get('hashedPassword') === hashedPassword) {
  return existingUser;
} else {
  throw new Error("Password is incorrect");
}
</code></pre>

<p>  }, function(error) {</p>

<pre><code>console.error('error in _hashPassword');
throw error;
</code></pre>

<p>  });
```</p>

<p>In the above example, if there is an error in <code>User.findOne</code>, the error is
propagated throughout the chain, which means that your console will have both
<code>error in findOne</code> and <code>error in _hashPassword</code> printed. There&rsquo;s no way
of &ldquo;breaking&rdquo; from the promise chain early. But here&rsquo;s the thing: you don&rsquo;t have to.
Unless you&rsquo;re interested in transforming error objects, there&rsquo;s not much
point in catching promise errors in intermediary steps in the chain. So the
above code would be abbreviated:</p>

<p>``` javascript
var promise = User.findOne({username: username})
  .then(function (user) {</p>

<pre><code>if (!user) {
  throw new Error("Username is incorrect");
}
existingUser = user;
return User._hashPassword(password, user.get('salt'));
</code></pre>

<p>  })
  .then(function (hashedPassword) {</p>

<pre><code>if (existingUser.get('hashedPassword') === hashedPassword) {
  return existingUser;
} else {
  throw new Error("Password is incorrect");
}
</code></pre>

<p>  });</p>

<p>//later in chain or in a diffent function
promise.then(function (eventualResult) {</p>

<pre><code>//do something with result
</code></pre>

<p>  }, function (error) {</p>

<pre><code>//handle error
</code></pre>

<p>  });
}
```</p>

<p>There&rsquo;s simply no point to having <code>then</code> failure handlers that just throw
an error, since the library throws and propagates errors for you automatically.
In promise chains, then, you have the benefit of isolating exceptions (only proceeding
from step to step if everything is ok) without the extra semantics of <code>if (err) return cb(err)</code>
from continuation passing.</p>
]]></content>
  </entry>
  
</feed>
